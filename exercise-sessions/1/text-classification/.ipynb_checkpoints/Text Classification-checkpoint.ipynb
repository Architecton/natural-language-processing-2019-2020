{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "Text classification is normally referred to as deriving a class for a text document according to precalculated features. A text document can be a paragraph, tweet post, sentence, forum post, ... On the other hand, text tagging problems deal with classification of tokens in a sequence. Sequences can represent tokens, sentences, forum thread, characters, ...\n",
    "\n",
    "<img src=\"classifiers.png\" width=\"600\">\n",
    "\n",
    "## Text Classification\n",
    "\n",
    "Some examples of text classification problems are text categorization, spam detection, sentiment analysis, language identification, ... Definition of a problem:\n",
    "\n",
    "**Input:**\n",
    "* a document $d \\in D$\n",
    "* a fixed set of classes $C=\\{c_1, c_2, c_3, ... c_j\\}$\n",
    "\n",
    "**Output:**\n",
    "* predicted class $c \\in C$\n",
    "\n",
    "Existing classifiers that can be used are naive Bayes, logistic regression, support-vector machines, k-nearest neighbours, classification tree, random forest, ...\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "One of the simplest classification methods based on a Bayes rule. It assumes features independence and therefore it can lack performance (e.g. XOR problem).\n",
    "\n",
    "$$\n",
    "P(c|d) = \\frac{P(d|c)P(c)}{P(d)} \\\\\n",
    "c_{MAP} = \\textrm{argmax}_{c \\in C}P(c|d) = \\textrm{argmax}_{c \\in C}P(d|c)P(c)\n",
    "$$\n",
    "\n",
    "For the classification we need to define the features - i.e. representation of an input document for a classifier. A simple approach would be to use a bag of words representation:\n",
    "\n",
    "<img src=\"bow1.png\" width=\"400\"> <img src=\"bow2.png\" width=\"400\">\n",
    "\n",
    "$$\n",
    "c_{MAP} = \\textrm{argmax}_{c \\in C}P(x_1, x_2, x_3, ..., x_n|c)P(c) \\\\\n",
    "c_{MAP} = \\textrm{argmax}_{c \\in C}P(x_1|c)P(x_2|c)P(x_3) ... P(x_n|c)P(c)\n",
    "$$\n",
    "\n",
    "Generally we need to classify examples into one of the multiple classes, therefore a  *multinomial* naive Bayes classifier is defined as:\n",
    "\n",
    "$$\n",
    "c_{NB} = \\textrm{argmax}_{c_j \\in C} P(c_j) \\prod_{i \\in \\textrm{features}} P(x_i|c_j)\n",
    "$$\n",
    "\n",
    "Basic code in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "### PSEUDOCODE ###\n",
    "##################\n",
    "\n",
    "# 1. Load data\n",
    "# 2. Prepare features\n",
    "features = []\n",
    "classes = []\n",
    "\n",
    "for example in examples:\n",
    "    item = []\n",
    "\n",
    "    # Our features (an idea)\n",
    "    item.append(hasLink(example))\n",
    "    item.append(frequency5w1h(example))\n",
    "    item.append(numberOfEmoticons(example))\n",
    "    item.append(numberOfUnicodeEmoticons(example))\n",
    "    item.append(numberOfInterjections(example))\n",
    "    item.append(containtsCapsWord(example))\n",
    "    item.append(verbPos(example))\n",
    "    item.append(numberOfNegations(example))\n",
    "    item.append(hasFutureVerb(example))\n",
    "    item += [hasPunctuations(example)[key] for key in punctuationKeys]\n",
    "    item.append(containsOneOf_5w1h(example))\n",
    "    item.append(textLength(example['tokens']))\n",
    "    item.append(textUniqueLength(example['tokens']))\n",
    "    item.append(textStemmedLength(example['tokens']))\n",
    "    item.append(textStemmedUniqueLength(example['tokens']))\n",
    "    item.append(postPos(example, examples))\n",
    "    item.append(sentencePos(example, examples))\n",
    "    item.append(hasQuestionmark(example))\n",
    "    item.append(hasExclamationMark(example))\n",
    "    item.append(hasThank(example))\n",
    "    item.append(hasPositiveFeedback(example))\n",
    "    item.append(sentimentValue(example))\n",
    "    item.append(sentenceNormPos(example, examples))\n",
    "    item.append(hasDuplicateWords(example))\n",
    "    item.append(isQuote(example, examples))\n",
    "    item.append(conversationSim(example, examples))\n",
    "    item.append(initSim(example, examples))\n",
    "\n",
    "    features.append(item)\n",
    "    classes.append(example['class'])\n",
    "\n",
    "(X, y) = (np.array(features), np.array(classes))\n",
    "print((\"Dataset shape: {}\".format((X.shape, y.shape))))\n",
    "\n",
    "# 3. Define classifier\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "\n",
    "# 4. Train a classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 5. Evaluate (cross-validation)\n",
    "scorings = [\"accuracy\", \"precision_weighted\", \"recall_weighted\", \"f1_weighted\"]\n",
    "for scoring in scorings:\n",
    "    scores = cross_val_score(clf, X, y, cv=10, scoring=scoring)\n",
    "\n",
    "# 6. Predict using a classifier\n",
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Classification of documents from 20 newsgroups dataset. Scikit-learn will download and cache the dataset in your home folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "\n",
    "# Loading 20 newsgroups dataset for categories\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=remove)\n",
    "\n",
    "target_names = data_train.target_names\n",
    "y_train, y_test = data_train.target, data_test.target\n",
    "\n",
    "# Extracting features from the data using a sparse vectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "\n",
    "X_train = vectorizer.fit_transform(data_train.data)\n",
    "X_test = vectorizer.transform(data_test.data)\n",
    "print(\"Training samples: %d, training features: %d\" % X_train.shape)\n",
    "print(\"Training samples: %d, training features: %d\" % X_test.shape)\n",
    "\n",
    "# Training and testing a Multinomial naive Bayes\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "\n",
    "print(\"Training ...\")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Testing ...\")\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"Classification accuracy:   %0.3f\" % score)\n",
    "\n",
    "print(\"Classification report:\")\n",
    "print(metrics.classification_report(y_test, pred,\n",
    "                                            target_names=target_names))\n",
    "\n",
    "# Show confusion matrix\n",
    "print(\"Confusion matrix:\")\n",
    "cm = metrics.confusion_matrix(y_test, pred)\n",
    "print(cm)\n",
    "plt.matshow(cm)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance measures\n",
    "\n",
    "Most common general measures for the classification methods in the machine learning are classification accuracy, precision, recall and F-score. \n",
    "\n",
    "Let's imagine we have a problem with two target classes. We would calculate performance of our classifier as follows:\n",
    "\n",
    "| - |Correct  |Not Correct  |\n",
    "|-------------|:-------:|:-----------:|\n",
    "|Selected     |TP       | FP          |\n",
    "|Not selected |FN       | TN          |\n",
    "\n",
    "Classification accuracy:\n",
    "$$\n",
    "\\textrm{CA} = \\frac{\\textrm{TP}+\\textrm{TN}}{\\textrm{TP}+\\textrm{FP}+\\textrm{FN}+\\textrm{TN}}\n",
    "$$\n",
    "\n",
    "Precision: a proportion of selected items that are correct\n",
    "$$\n",
    "\\textrm{P} = \\frac{\\textrm{TP}}{\\textrm{TP}+\\textrm{FP}}\n",
    "$$\n",
    "\n",
    "Recall: a proportion of correct items that are selected\n",
    "$$\n",
    "\\textrm{R} = \\frac{\\textrm{TP}}{\\textrm{TP}+\\textrm{FN}}\n",
    "$$\n",
    "\n",
    "F (i.e. $F_1$) measure: a harmonic mean (i.e., very conservative average) between precision and recall\n",
    "$$\n",
    "\\textrm{F} = \\frac{(\\beta^2+1)\\textrm{PR}}{\\beta^2\\textrm{P}+\\textrm{R}}; \\textrm{F}_1 = \\frac{2\\textrm{PR}}{\\textrm{P}+\\textrm{R}}\n",
    "$$\n",
    "\n",
    "If we have a multi-class problem, we need to take this into account also for the evaluation:\n",
    "* Macro-averaging: compute performance for each class, then average\n",
    "* Micro-averaging: collect decisions for all classes, compute contingency table, evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercises\n",
    "\n",
    "* Experiment with *20 newsgroups* dataset on your own (manually download and import, remove headers, adjust parameters, train and test on the same data :), ...).\n",
    "* Experiment with other classifiers: Logistic regression, Random forest, SVM, ...\n",
    "* Implement your own features.\n",
    "* Try to play with some other dataset or create your own. For example, use [Reuters-21578](http://www.daviddlewis.com/resources/testcollections/reuters21578/) - example [here](http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-text-classification-1.html).\n",
    "* Implement F-score (also MaF and MiF) performance measures on your own."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
